apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: obslytics-data-exporter-workflow-template
  labels:
    owner: obslytics-remote-reader
    app: obslytics-data-exporter
    pipeline: obslytics-data-exporter
spec:
  templates:
    - name: init
      parallelism: 2
      inputs:
        parameters:
          - name: metrics_list
          - name: start_timestamp
          - name: step
          - name: delay
          - name: resolution
          - name: worker_size
      steps:
        - - name: metric-split
            template: metric-worker
            arguments:
              parameters:
                - name: metric
                  value: "{{item}}"
                - name: start_timestamp
                  value: "{{inputs.parameters.start_timestamp}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
                - name: resolution
                  value: "{{inputs.parameters.resolution}}"
                - name: worker_size
                  value: "{{inputs.parameters.worker_size}}"
            withParam: "{{inputs.parameters.metrics_list}}"
    - name: metric-worker
      parallelism: 1
      inputs:
        parameters:
          - name: metric
          - name: start_timestamp
          - name: step
          - name: delay
          - name: resolution
          - name: worker_size
      dag:
        tasks:
          - name: process-metric-small
            template: run-obslytics-small
            arguments:
              parameters:
                - name: metric
                  value: "{{inputs.parameters.metric}}"
                - name: start_timestamp
                  value: "{{inputs.parameters.start_timestamp}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
                - name: resolution
                  value: "{{inputs.parameters.resolution}}"
                - name: dryrun
                  value: ""
            when: "{{inputs.parameters.worker_size}} == 'small'"
          - name: process-metric-large
            template: run-obslytics-large
            arguments:
              parameters:
                - name: metric
                  value: "{{inputs.parameters.metric}}"
                - name: start_timestamp
                  value: "{{inputs.parameters.start_timestamp}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
                - name: resolution
                  value: "{{inputs.parameters.resolution}}"
                - name: dryrun
                  value: ""
            when: "{{inputs.parameters.worker_size}} == 'large'"
          - name: process-metric-xl
            template: run-obslytics-xl
            arguments:
              parameters:
                - name: metric
                  value: "{{inputs.parameters.metric}}"
                - name: start_timestamp
                  value: "{{inputs.parameters.start_timestamp}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
                - name: resolution
                  value: "{{inputs.parameters.resolution}}"
                - name: dryrun
                  value: ""
            when: "{{inputs.parameters.worker_size}} == 'xl'"

    - name: normalize-timestamps
      activeDeadlineSeconds: 600
      inputs:
        parameters:
          - name: backfill_from_timestamp
          - name: backfill_min_ts
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-data-exporter:latest
        command: [/bin/bash, -e]
        source: |
          date --date="{{inputs.parameters.backfill_from_timestamp}}" +%FT%TZ -u > /mnt/out/backfill_from_timestamp
          date --date="{{inputs.parameters.backfill_min_ts}}" +%FT%TZ -u > /mnt/out/backfill_min_ts

        volumeMounts:
          - name: out
            mountPath: /mnt/out
      volumes:
        - name: out
          emptyDir: {}
      outputs:
        parameters:
          - name: backfill_from_timestamp
            valueFrom:
              path: /mnt/out/backfill_from_timestamp
          - name: backfill_min_ts
            valueFrom:
              path: /mnt/out/backfill_min_ts
      resources:
        limits:
          memory: 10Gi


    - name: get-timestamp-list
      activeDeadlineSeconds: 600
      inputs:
        parameters:
          - name: step
          - name: metric
          - name: backfill_count
          - name: backfill_from_timestamp
          - name: min_ts
          - name: delay
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-pyscripts:latest
        command: [/bin/bash, -e]
        source: |
          set -x
          python py/gap_detector.py \
              --storage_config /etc/obslytics-data-exporter/storage-config.yaml \
              --prefix data/ \
              --metric {{inputs.parameters.metric}}/ \
              --step {{inputs.parameters.step}} \
              --delay {{inputs.parameters.delay}} \
              --from_timestamp {{inputs.parameters.backfill_from_timestamp}} \
              --earliest_timestamp {{inputs.parameters.min_ts}} \
              --max_timestamps {{inputs.parameters.backfill_count}} \
              --output_file /mnt/out/tslist \
              --verbose \
              --backfill

        volumeMounts:
          - name: out
            mountPath: /mnt/out
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter
      volumes:
        - name: out
          emptyDir: {}
      outputs:
        parameters:
          - name: tslist
            valueFrom:
              path: /mnt/out/tslist
      resources:
        limits:
          memory: 10Gi
        requests:
          memory: 10Gi

    - name: get-labels
      retryStrategy:
        limit: 5
      activeDeadlineSeconds: 600
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-data-exporter:latest
        command: [/bin/sh, -e]
        source: |
          curl --silent --insecure https://telemeter-lts.datahub.redhat.com/api/v1/label/__name__/values -H "Authorization: Bearer `cat /etc/obslytics-data-exporter/satoken.txt`" > /tmp/labels.json
          yq r /tmp/labels.json data
        resources:
          limits:
            memory: 10Gi
        volumeMounts:
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter

    - name: run-obslytics-large
      retryStrategy:
        limit: 5
      inputs:
        parameters:
          - name: metric
          - name: start_timestamp
          - name: step
          - name: delay
          - name: resolution
          - name: dryrun
      activeDeadlineSeconds: 600
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-data-exporter:latest
        command: [/bin/bash, -e]
        source: |
          set -x
          export GOGC=30

          echo "Calling Script with Params:"
          epoch=`date --date="{{inputs.parameters.start_timestamp}}" +"%s"`
          echo "START       : ${epoch} ({{inputs.parameters.start_timestamp}})"
          echo "METRIC      : {{inputs.parameters.metric}}"
          echo "STEP        : {{inputs.parameters.step}}"
          echo "DELAY       : {{inputs.parameters.delay}}"
          echo "RESOLUTION  : {{inputs.parameters.resolution}}"
          echo "PREFIX      : data/"

          # Config params
          export INPUT_CONFIG_FILE="/etc/obslytics-data-exporter/input-config.yaml"
          export STORAGE_CONFIG_FILE="/etc/obslytics-data-exporter/storage-config.yaml"

          # Input Params
          export METRIC_MAX_TIME="${epoch}"
          export METRIC_STEP="{{inputs.parameters.step}}"
          export METRIC_DELAY="{{inputs.parameters.delay}}"
          export METRIC_MATCH="{{inputs.parameters.metric}}"

          # Storage Params
          export OUTPUT_FILE_TYPE="parquet"
          export OUTPUT_METRIC_RESOLUTION="{{inputs.parameters.resolution}}"
          export METRIC_STORAGE_PREFIX="data/"

          # Mode Parameters
          export VERIFY_DATA_STORED="false"
          export DRYRUN="{{inputs.parameters.dryrun}}"

          bash ./run.sh
          echo "Export Complete."

        resources:
          request:
            memory: 10Gi
          limits:
            memory: 10Gi
        volumeMounts:
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter


    # This workflow template will backfill a single metric.
    - name: backfill-metric
      parallelism: 1
      inputs:
        parameters:
          - name: backfill_count
          - name: backfill_from_timestamp
          - name: delay
          - name: metric
          - name: resolution
          - name: step
          - name: backfill_min_ts
          - name: worker_size
      steps:
        - - name: get-timestamps
            arguments:
              parameters:
                - name: backfill_count
                  value: "{{inputs.parameters.backfill_count}}"
                - name: backfill_from_timestamp
                  value: "{{inputs.parameters.backfill_from_timestamp}}"
                - name: metric
                  value: "{{inputs.parameters.metric}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: min_ts
                  value: "{{inputs.parameters.backfill_min_ts}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
            templateRef:
              name: "obslytics-data-exporter-workflow-template"
              template: get-timestamp-list

        - - name: run
            arguments:
              parameters:
                - name: delay
                  value: "0"  # DO NOT change, this is a delay off of the already detected gaps, and should always be 0.
                - name: metrics_list
                  value: "[\"{{inputs.parameters.metric}}\"]"
                - name: start_timestamp
                  value: "{{item}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: resolution
                  value: "5m"
                - name: worker_size
                  value: "{{inputs.parameters.worker_size}}"
            templateRef:
              name: "obslytics-data-exporter-workflow-template"
              template: init
            withParam: "{{steps.get-timestamps.outputs.parameters.tslist}}"

    # This workflow template will verify a single metric.
    - name: verify
      activeDeadlineSeconds: 600
      inputs:
        parameters:
          - name: step
          - name: verify_count
          - name: verification_from_timestamp
          - name: delay
          - name: metric
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-pyscripts:latest
        command: [/bin/bash, -e]
        source: |
          set -x
          python py/verify.py \
              --config_file /etc/obslytics-data-exporter/storage-config.yaml \
              --prefix data/ \
              --metric {{inputs.parameters.metric}} \
              --step {{inputs.parameters.step}} \
              --from_timestamp {{inputs.parameters.verification_from_timestamp}} \
              --max_timestamps {{inputs.parameters.verify_count}} \
              --delay {{inputs.parameters.delay}} \

        volumeMounts:
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter
      resources:
        limits:
          memory: 2Gi
        requests:
          memory: 2Gi

    - name: extract-partitioned-table-names
      script:
        image: "quay.io/opendatahub/trino:356"
        env:
          - name: TRINO_USER
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_USER
          - name: TRINO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_PASSWORD
          - name: TRINO_HOST
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_HOST
        command: [python]
        source: |
          import trino
          import json
          import urllib3
          import os

          # Temporarily disabling warnings from urllib3
          urllib3.disable_warnings()

          SHOW_CREATE_TABLES="""
          select
              table_schema,
              table_name,
              'show create table '||table_catalog||'.'||table_schema||'.'||table_name
          from
              hive.information_schema.tables
          where
              table_schema <> 'information_schema'
          """

          conn = trino.dbapi.connect(
              host=os.environ["TRINO_HOST"],
              port=443,
              user=os.environ["TRINO_USER"],
              catalog='hive',
              http_scheme='https',
              auth=trino.auth.BasicAuthentication(os.environ["TRINO_USER"], os.environ["TRINO_PASSWORD"])
          )
          cur = conn.cursor()
          cur.execute(SHOW_CREATE_TABLES)
          rows = cur.fetchall()

          schemas = []

          for row in rows:
              create_table_sql = cur.execute(row[2])
              if "partitioned_by" in cur.fetchall()[0][0]:
                  print("Adding {}.{} to the list of partitioned tables...".format(row[0],row[1]))

                  schema = next((item for item in schemas if item["name"] == row[0]), None)


                  if schema:
                      schema["tables"].append(row[1])
                  else:
                      schema = {}
                      schema["name"] = row[0]
                      schema["tables"] = []
                      schema["tables"].append(row[1])
                      schemas.append(schema)

          with open('/mnt/out/tbl_list','w') as f:
              print("Dumping table list as json data...")
              json.dump(schemas, f)

        volumeMounts:
          - name: out
            mountPath: /mnt/out
      volumes:
        - name: out
          emptyDir: {}
      outputs:
        parameters:
          - name: tables_list
            valueFrom:
              path: /mnt/out/tbl_list

    - name: update-partition-metadata
      inputs:
        parameters:
          - name: table_defs
      script:
        image: "quay.io/opendatahub/trino:356"
        env:
          - name: TRINO_USER
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_USER
          - name: TRINO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_PASSWORD
          - name: TRINO_HOST
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_HOST
        command: [python]
        source: |
          import trino
          import json
          import urllib3
          import os
          import sys

          # Disable warnings from urllib3
          urllib3.disable_warnings()

          TABLE_DEFS = '{{inputs.parameters.table_defs}}'

          UPDATE_PARTITION = "call hive.system.sync_partition_metadata('{}','{}','FULL')"

          conn = trino.dbapi.connect(
              host=os.environ["TRINO_HOST"],
              port=443,
              user=os.environ["TRINO_USER"],
              catalog='hive',
              http_scheme='https',
              auth=trino.auth.BasicAuthentication(os.environ["TRINO_USER"], os.environ["TRINO_PASSWORD"])
          )

          table_definitions = json.loads(TABLE_DEFS)

          schema_name = table_definitions["name"]
          schema_tables = table_definitions["tables"]

          print("Begin partition metadata update for schema {}...".format(schema_name))
          for table in schema_tables:
              cur = conn.cursor()
              sys.stdout.write("Updating partition metadata update for table {}...".format(table))
              cur.execute(UPDATE_PARTITION.format(schema_name, table))
              cur.fetchone()
              sys.stdout.write("Done!")
              sys.stdout.flush()

    - name: run-obslytics-small
      retryStrategy:
        limit: 5
      inputs:
        parameters:
          - name: metric
          - name: start_timestamp
          - name: step
          - name: delay
          - name: resolution
          - name: dryrun
      activeDeadlineSeconds: 600
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-data-exporter:latest
        command: [/bin/bash, -e]
        source: |
          set -x
          export GOGC=30

          echo "Calling Script with Params:"
          epoch=`date --date="{{inputs.parameters.start_timestamp}}" +"%s"`
          echo "START       : ${epoch} ({{inputs.parameters.start_timestamp}})"
          echo "METRIC      : {{inputs.parameters.metric}}"
          echo "STEP        : {{inputs.parameters.step}}"
          echo "DELAY       : {{inputs.parameters.delay}}"
          echo "RESOLUTION  : {{inputs.parameters.resolution}}"
          echo "PREFIX      : data/"

          # Config params
          export INPUT_CONFIG_FILE="/etc/obslytics-data-exporter/input-config.yaml"
          export STORAGE_CONFIG_FILE="/etc/obslytics-data-exporter/storage-config.yaml"

          # Input Params
          export METRIC_MAX_TIME="${epoch}"
          export METRIC_STEP="{{inputs.parameters.step}}"
          export METRIC_DELAY="{{inputs.parameters.delay}}"
          export METRIC_MATCH="{{inputs.parameters.metric}}"

          # Storage Params
          export OUTPUT_FILE_TYPE="parquet"
          export OUTPUT_METRIC_RESOLUTION="{{inputs.parameters.resolution}}"
          export METRIC_STORAGE_PREFIX="data/"

          # Mode Parameters
          export VERIFY_DATA_STORED="false"
          export DRYRUN="{{inputs.parameters.dryrun}}"

          bash ./run.sh
          echo "Export Complete."

        resources:
          request:
            memory: 4Gi
          limits:
            memory: 4Gi
        volumeMounts:
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter

    - name: run-obslytics-xl
      retryStrategy:
        limit: 5
      inputs:
        parameters:
          - name: metric
          - name: start_timestamp
          - name: step
          - name: delay
          - name: resolution
          - name: dryrun
      activeDeadlineSeconds: 600
      script:
        image: docker-registry.default.svc:5000/dh-prod-thanos-extractor/obslytics-data-exporter:latest
        command: [/bin/bash, -e]
        source: |
          set -x
          export GOGC=30

          echo "Calling Script with Params:"
          epoch=`date --date="{{inputs.parameters.start_timestamp}}" +"%s"`
          echo "START       : ${epoch} ({{inputs.parameters.start_timestamp}})"
          echo "METRIC      : {{inputs.parameters.metric}}"
          echo "STEP        : {{inputs.parameters.step}}"
          echo "DELAY       : {{inputs.parameters.delay}}"
          echo "RESOLUTION  : {{inputs.parameters.resolution}}"
          echo "PREFIX      : data/"

          # Config params
          export INPUT_CONFIG_FILE="/etc/obslytics-data-exporter/input-config.yaml"
          export STORAGE_CONFIG_FILE="/etc/obslytics-data-exporter/storage-config.yaml"

          # Input Params
          export METRIC_MAX_TIME="${epoch}"
          export METRIC_STEP="{{inputs.parameters.step}}"
          export METRIC_DELAY="{{inputs.parameters.delay}}"
          export METRIC_MATCH="{{inputs.parameters.metric}}"

          # Storage Params
          export OUTPUT_FILE_TYPE="parquet"
          export OUTPUT_METRIC_RESOLUTION="{{inputs.parameters.resolution}}"
          export METRIC_STORAGE_PREFIX="data/"

          # Mode Parameters
          export VERIFY_DATA_STORED="false"
          export DRYRUN="{{inputs.parameters.dryrun}}"

          bash ./run.sh
          echo "Export Complete."

        resources:
          request:
            memory: 16Gi
          limits:
            memory: 16Gi
        volumeMounts:
          - name: obslytics-data-exporter-workflow-secrets
            mountPath: /etc/obslytics-data-exporter

    # The same as the regular backfill-metric template, but with 6x timestamp parallelism
    - name: backfill-small-metric
      parallelism: 6
      inputs:
        parameters:
          - name: backfill_count
          - name: backfill_from_timestamp
          - name: delay
          - name: metric
          - name: resolution
          - name: step
          - name: backfill_min_ts
          - name: worker_size
      steps:
        - - name: get-timestamps
            arguments:
              parameters:
                - name: backfill_count
                  value: "{{inputs.parameters.backfill_count}}"
                - name: backfill_from_timestamp
                  value: "{{inputs.parameters.backfill_from_timestamp}}"
                - name: metric
                  value: "{{inputs.parameters.metric}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: min_ts
                  value: "{{inputs.parameters.backfill_min_ts}}"
                - name: delay
                  value: "{{inputs.parameters.delay}}"
            templateRef:
              name: "obslytics-data-exporter-workflow-template"
              template: get-timestamp-list

        - - name: run
            arguments:
              parameters:
                - name: delay
                  value: "0"  # DO NOT change, this is a delay off of the already detected gaps, and should always be 0.
                - name: metrics_list
                  value: "[\"{{inputs.parameters.metric}}\"]"
                - name: start_timestamp
                  value: "{{item}}"
                - name: step
                  value: "{{inputs.parameters.step}}"
                - name: resolution
                  value: "5m"
                - name: worker_size
                  value: "{{inputs.parameters.worker_size}}"
            templateRef:
              name: "obslytics-data-exporter-workflow-template"
              template: init
            withParam: "{{steps.get-timestamps.outputs.parameters.tslist}}"
